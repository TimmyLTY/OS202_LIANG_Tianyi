# Travaux DirigÃ©s - OS02 ParallÃ©lisme

**Auteur :** LIANG Tianyi  
**Cours :** OS02 - Calcul ParallÃ¨le et DistribuÃ©  
**Ã‰tablissement :** ENSTA Paris  
**AnnÃ©e :** 2025-2026

---

## ğŸ“‹ Table des matiÃ¨res

| TP | ThÃ¨me | Technologies | Rapport |
|:--:|-------|--------------|---------|
| [TP1](tp1/) | Produit matrice-matrice & ParallÃ©lisation OpenMP | C++, OpenMP, BLAS | [TP1_Rapport.md](tp1/TP1_Rapport.md) |
| [TP2](tp2/) | ParallÃ©lisation MPI : Mandelbrot & Produit matrice-vecteur | Python, MPI, mpi4py | [TP2_Rapport.md](tp2/TP2_Rapport.md) |
| [TP3](tp3/) | Bucket Sort ParallÃ¨le avec MPI | C++, MPI | [TP3_Rapport.md](tp3/TP3_Rapport.md) |
| [TP4](tp4/) | Jeu de la Vie â€” ParallÃ©lisation MPI | Python, MPI, mpi4py, pygame | [TP4_Rapport.md](tp4/TP4_Rapport.md) |
| [TP5](tp5/) | Calcul GPU avec PyCUDA | Python, PyCUDA, CUDA, Google Colab | [TP5_Rapport.md](tp5/TP5_Rapport.md) |

---

## ğŸ“ Structure des dossiers

```
travaux_diriges/
â”œâ”€â”€ README.md                    # Ce fichier (index principal)
â”œâ”€â”€ tp1/
â”‚   â”œâ”€â”€ TP1_Rapport.md          # Rapport complet TP1
â”‚   â”œâ”€â”€ Sujet.pdf               # Ã‰noncÃ© du TP1
â”‚   â”œâ”€â”€ sources/                # Code source C++
â”‚   â”‚   â”œâ”€â”€ ProdMatMat.cpp      # Produit matrice-matrice optimisÃ©
â”‚   â”‚   â”œâ”€â”€ Matrix.hpp          # Classe matrice
â”‚   â”‚   â”œâ”€â”€ TestProductMatrix.cpp
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ solution/               # Solutions additionnelles
â”‚       â”œâ”€â”€ jeton_anneau.py     # Circulation de jeton MPI
â”‚       â””â”€â”€ calcul_pi.cpp       # Calcul de Ï€ parallÃ¨le
â”‚
â”œâ”€â”€ tp2/
â”‚   â”œâ”€â”€ TP2_Rapport.md          # Rapport complet TP2
â”‚   â”œâ”€â”€ Readme.md               # Ã‰noncÃ© du TP2
â”‚   â”œâ”€â”€ mandelbrot_block.py     # StratÃ©gie partition par blocs
â”‚   â”œâ”€â”€ mandelbrot_cyclic.py    # StratÃ©gie rÃ©partition cyclique
â”‚   â”œâ”€â”€ mandelbrot_master_slave.py # StratÃ©gie maÃ®tre-esclave
â”‚   â”œâ”€â”€ matvec_col.py           # Produit matrice-vecteur par colonnes
â”‚   â”œâ”€â”€ matvec_row.py           # Produit matrice-vecteur par lignes
â”‚   â”œâ”€â”€ plot_results.py         # GÃ©nÃ©ration des graphiques
â”‚   â”œâ”€â”€ run_all_tp2_experiments.sh # Script d'automatisation
â”‚   â”œâ”€â”€ images/                 # Images Mandelbrot gÃ©nÃ©rÃ©es
â”‚   â”œâ”€â”€ plots/                  # Graphiques de performance
â”‚   â””â”€â”€ results/                # RÃ©sultats expÃ©rimentaux
â”‚
â”œâ”€â”€ tp3/
â”‚   â”œâ”€â”€ TP3_Rapport.md          # Rapport complet TP3
â”‚   â”œâ”€â”€ README.md               # Ã‰noncÃ© et instructions
â”‚   â”œâ”€â”€ sources/                # Code source C++
â”‚   â”‚   â”œâ”€â”€ bucket_sort_seq.cpp     # Version sÃ©quentielle
â”‚   â”‚   â”œâ”€â”€ bucket_sort_mpi.cpp     # Version parallÃ¨le MPI
â”‚   â”‚   â”œâ”€â”€ Makefile                # Compilation
â”‚   â”‚   â””â”€â”€ run_experiments.sh      # Script d'automatisation
â”‚   â”œâ”€â”€ results/                # RÃ©sultats expÃ©rimentaux
â”‚   â””â”€â”€ images/                 # Illustrations du cours
â”‚
â”œâ”€â”€ tp4/
â”‚   â”œâ”€â”€ TP4_Rapport.md          # Rapport complet TP4
â”‚   â”œâ”€â”€ game_of_life.py         # Jeu de la Vie parallÃ¨le (MPI)
â”‚   â”œâ”€â”€ game_of_life_parallel.py # Copie identique
â”‚   â”œâ”€â”€ benchmark_headless.py   # Benchmark sans affichage
â”‚   â””â”€â”€ benchmark_results.csv   # RÃ©sultats expÃ©rimentaux
â”‚
â””â”€â”€ tp5/
    â”œâ”€â”€ TP5_Rapport.md          # Rapport complet TP5
    â”œâ”€â”€ TP5_LIANG_Tianyi.ipynb   # Notebook Colab exÃ©cutÃ©
    â”œâ”€â”€ TP_numero_cinq.ipynb     # Notebook original du cours
    â””â”€â”€ test_numba/              # Exemples Numba (prÃ©paration exam)
```

---

## ğŸ“– RÃ©sumÃ© des TPs

### TP1 : Produit Matrice-Matrice et OpenMP

**Objectifs :**
- Comprendre l'impact de l'ordre des boucles sur les performances (cache)
- Optimiser avec la technique de blocking
- Comparer avec la bibliothÃ¨que BLAS optimisÃ©e
- ParallÃ©liser avec OpenMP

**RÃ©sultats clÃ©s :**
- L'ordre `ikj` est optimal (accÃ¨s mÃ©moire contigus)
- Le blocking amÃ©liore les performances de ~15%
- OpenMP atteint une efficacitÃ© de ~85% sur 4 threads
- BLAS surpasse toutes les implÃ©mentations manuelles (3200 GFLOPS vs 200 GFLOPS)

### TP2 : ParallÃ©lisation MPI

**Objectifs :**
- ParallÃ©liser le calcul de l'ensemble de Mandelbrot avec 3 stratÃ©gies
- ImplÃ©menter le produit matrice-vecteur distribuÃ©
- Analyser les lois d'Amdahl et Gustafson

**RÃ©sultats clÃ©s :**
- **Partition par blocs** : Speedup 5.31Ã— avec 8 processus (efficacitÃ© 66%)
- **RÃ©partition cyclique** : Speedup 6.10Ã— (efficacitÃ© 76%) - meilleur Ã©quilibrage
- **MaÃ®tre-esclave** : EfficacitÃ© 82% - meilleure adaptation Ã  la charge
- Le dÃ©sÃ©quilibre de charge vient de la complexitÃ© variable du calcul Mandelbrot

### TP3 : Bucket Sort ParallÃ¨le

**Objectifs :**
- ImplÃ©menter l'algorithme Bucket Sort parallÃ¨le avec MPI
- Utiliser l'approche Sample Sort pour l'Ã©quilibrage de charge
- MaÃ®triser les communications All-to-All

**Points clÃ©s :**
- **Sample Sort** : Ã‰chantillonnage pour dÃ©finir des frontiÃ¨res de buckets Ã©quilibrÃ©es
- Utilisation de `MPI_Scatterv`, `MPI_Alltoallv`, `MPI_Gatherv`
- **Merge k-way** avec heap pour fusionner les listes triÃ©es
- Analyse de performance et scalabilitÃ©

### TP4 : Jeu de la Vie â€” ParallÃ©lisation MPI

**Objectifs :**
- ParallÃ©liser l'automate cellulaire Â« Game of Life Â» sur grille torique
- ImplÃ©menter la dÃ©composition de domaine par bandes horizontales
- GÃ©rer l'Ã©change de cellules fantÃ´mes (ghost cells) entre processus
- SÃ©parer contrÃ´leur (affichage) et workers (calcul)

**RÃ©sultats clÃ©s :**
- Vectorisation (`np.roll`) vs boucles Python : **accÃ©lÃ©ration ~137Ã—**
- ParallÃ¨le (grille 400Ã—400, 5000 itÃ©rations) : speedup max **1.61Ã—** avec 4 workers
- Communication (ghost + Gatherv) domine Ã  8 workers (54% du temps total)
- Le ratio calcul/communication limite l'efficacitÃ© sur petites grilles

### TP5 : Calcul GPU avec PyCUDA

**Objectifs :**
- Programmer des kernels CUDA en Python via PyCUDA sur Google Colab (Tesla T4)
- Comprendre l'indexation des threads et blocs CUDA (1D et 2D)
- Comparer les performances CPU (NumPy) vs GPU (CUDA)

**RÃ©sultats clÃ©s :**
- Addition vectorielle (N=10M) : **speedup 33.64Ã—**
- Mandelbrot (1000Ã—1000, 100 itÃ©rations) : **speedup 5235Ã—**
- Mandelbrot haute rÃ©solution (4000Ã—4000, 200 itÃ©rations) : **speedup 13 497Ã—**
- Le speedup GPU/CPU augmente avec la taille du problÃ¨me (paradigme SIMT)

---

## ğŸ› ï¸ Environnement de dÃ©veloppement

### TP1 (C++/OpenMP)
```bash
# Compilation
make all

# ExÃ©cution
./TestProductMatrix.exe 1024
```

### TP2 (Python/MPI)
```bash
# Installation des dÃ©pendances
pip install numpy mpi4py pillow matplotlib

# ExÃ©cution MPI
mpirun -np 4 python3 mandelbrot_block.py
```

### TP3 (C++/MPI)
```bash
cd tp3/sources

# Compilation
make all

# ExÃ©cution
./bucket_sort_seq.exe 1000000                    # Version sÃ©quentielle
mpirun -np 4 ./bucket_sort_mpi.exe 1000000       # Version parallÃ¨le
```

### TP4 (Python/MPI/pygame)
```bash
# Simulation avec affichage (1 controller + 3 workers)
mpirun -np 4 python3 tp4/game_of_life.py glider_gun

# Benchmark headless
mpirun -np 4 python3 tp4/benchmark_headless.py --steps 5000 --pattern block_switch_engine
```

### TP5 (Python/PyCUDA â€” Google Colab)
```bash
# ExÃ©cuter le notebook TP5_LIANG_Tianyi.ipynb sur Google Colab
# PrÃ©requis : activer le runtime GPU (Tesla T4)
# Le notebook installe PyCUDA automatiquement via pip
```

---

## ğŸ“Š RÃ©sultats expÃ©rimentaux

Les rÃ©sultats dÃ©taillÃ©s sont disponibles dans chaque rapport :
- [RÃ©sultats TP1](tp1/TP1_Rapport.md#rÃ©sultats)
- [RÃ©sultats TP2](tp2/TP2_Rapport.md#rÃ©sultats-expÃ©rimentaux)
- [RÃ©sultats TP3](tp3/TP3_Rapport.md#rÃ©sultats-expÃ©rimentaux)
- [RÃ©sultats TP4](tp4/TP4_Rapport.md#3-rÃ©sultats-expÃ©rimentaux)
- [RÃ©sultats TP5](tp5/TP5_Rapport.md)

---

## ğŸ“š RÃ©fÃ©rences

- Support de cours : [transparents/](../transparents/)
- Exemples MPI : [Exemples/MPI/](../Exemples/MPI/)
- Documentation MPI : https://mpi4py.readthedocs.io/

---

*DerniÃ¨re mise Ã  jour : FÃ©vrier 2026*
